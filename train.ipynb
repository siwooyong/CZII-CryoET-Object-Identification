{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-cNuu_WMuwV"
      },
      "source": [
        "## hardware"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LK0Mg5TdMoZP"
      },
      "outputs": [],
      "source": [
        "!cat /proc/cpuinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRo_6MfbM2jb"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQBAR1gnMyS2"
      },
      "source": [
        "## kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez38RhS0M8pD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    if not os.path.isfile('/content/drive/MyDrive/Kaggle/CZII - CryoET Object Identification/data/czii-cryo-et-object-identification.zip'):\n",
        "        !mkdir -p ~/.kaggle\n",
        "        !cp /content/drive/MyDrive/Kaggle/kaggle.json ~/.kaggle/\n",
        "        !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "        !kaggle competitions download -c czii-cryo-et-object-identification -p '/content/drive/MyDrive/Kaggle/CZII - CryoET Object Identification/data/'\n",
        "\n",
        "    if not os.path.isdir('/content/train/'):\n",
        "        !unzip '/content/drive/MyDrive/Kaggle/CZII - CryoET Object Identification/data/czii-cryo-et-object-identification.zip'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuemSOaegL1d"
      },
      "source": [
        "## library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDoewkSXgL1e"
      },
      "outputs": [],
      "source": [
        "#!pip install zarr copick timm segmentation_models_pytorch connected-components-3d monai\n",
        "################################################################################\n",
        "!pip uninstall torch -y\n",
        "!pip install mmengine mmcv zarr copick timm segmentation_models_pytorch connected-components-3d monai torch==2.4.0\n",
        "################################################################################\n",
        "!pip install git+https://github.com/copick/copick-utils.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIBj0krmgL1e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import random\n",
        "\n",
        "import glob\n",
        "\n",
        "import json\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "\n",
        "import albumentations as A\n",
        "\n",
        "import zarr\n",
        "\n",
        "import copick\n",
        "\n",
        "import timm\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "import cc3d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fogV58zPnZU"
      },
      "source": [
        "## config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNpH8bbAPogA"
      },
      "outputs": [],
      "source": [
        "class CustomConfig:\n",
        "    seed = 42\n",
        "    device = 'cuda'\n",
        "    root = '/content/drive/MyDrive/Kaggle/CZII - CryoET Object Identification/'\n",
        "\n",
        "    radius_scale = 0.5\n",
        "\n",
        "    n_fold = 7\n",
        "\n",
        "    filters = [\n",
        "        'denoised',\n",
        "        'wbp',\n",
        "        'ctfdeconvolved',\n",
        "        'isonetcorrected'\n",
        "        ]\n",
        "\n",
        "    particle_types = [\n",
        "        'apo-ferritin',\n",
        "        'beta-amylase',\n",
        "        'beta-galactosidase',\n",
        "        'ribosome',\n",
        "        'thyroglobulin',\n",
        "        'virus-like-particle',\n",
        "        ]\n",
        "\n",
        "    particle2class = dict(zip(particle_types, np.arange(1, len(particle_types) + 1)))\n",
        "\n",
        "    tomogram_path = '/content/train/static/ExperimentRuns/'\n",
        "    segmentation_path = '/content/overlay/ExperimentRuns/'\n",
        "\n",
        "    tomogram_name = 'VoxelSpacing10.000'\n",
        "    segmentation_name = 'Segmentations/10.000_copickUtils_0_paintedPicks-multilabel'\n",
        "\n",
        "    volume_size = [192, 640, 640]\n",
        "    patch_size = [64, 256, 256]\n",
        "    stride_size = [32, 128, 128]\n",
        "\n",
        "    size_scale = [\n",
        "        184 / volume_size[0],\n",
        "        630 / volume_size[1],\n",
        "        630 / volume_size[2],\n",
        "    ]\n",
        "\n",
        "    offset = []\n",
        "    for i in range(0, volume_size[0] - patch_size[0] + 1, stride_size[0]):\n",
        "        for j in range(0, volume_size[1] - patch_size[1] + 1, stride_size[1]):\n",
        "            for k in range(0, volume_size[2] - patch_size[2] + 1, stride_size[2]):\n",
        "                offset.append([i, j, k])\n",
        "\n",
        "    clip_range = (1, 99)\n",
        "\n",
        "    model_version = 1\n",
        "    n_channel = 1\n",
        "    n_class = 7\n",
        "\n",
        "    model_name = 'resnet18d.ra2_in1k'\n",
        "\n",
        "    if model_name in [\n",
        "        'resnet18d.ra2_in1k',\n",
        "        'resnet34d.ra2_in1k',\n",
        "        'tf_efficientnet_b1.in1k',\n",
        "    ]:\n",
        "        drop_path_rate = 0.1\n",
        "        n_block = 5\n",
        "        decoder_channels = [256, 128, 64, 32, 32]\n",
        "\n",
        "\n",
        "    elif model_name == 'r50ir':\n",
        "        n_block = 4\n",
        "        encoder_channels = [256, 512, 1024, 2048]\n",
        "        decoder_channels = [256, 128, 64, 32]\n",
        "        pretrained_path = 'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r50_ig65m_20210617-ce545a37.pth'\n",
        "\n",
        "    elif model_name == 'r152ir':\n",
        "        n_block = 4\n",
        "        encoder_channels = [256, 512, 1024, 2048]\n",
        "        decoder_channels = [256, 128, 64, 32]\n",
        "        pretrained_path = 'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r152_ig65m_20200807-771c4135.pth'\n",
        "\n",
        "    else:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    n_epoch = 30\n",
        "    batch_size = 4\n",
        "    test_batch_size = 4\n",
        "    iters_to_accumulate = 1\n",
        "\n",
        "    n_worker = os.cpu_count()\n",
        "\n",
        "    lr = 1e-3\n",
        "    wd = 1e-2\n",
        "    warmup_ratio = 0.1\n",
        "    test_freq = 2\n",
        "\n",
        "    voxel_space = 10.0\n",
        "    thresholds = [0.10, 0.25, 0.50, 0.75, 0.90]\n",
        "\n",
        "    tta = []\n",
        "\n",
        "    mix_prob = 0.5\n",
        "    mix_alpha = 1.0\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = CustomConfig()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yqi9nGb9PosD"
      },
      "source": [
        "## seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoJF9qlrPq0M"
      },
      "outputs": [],
      "source": [
        "def seed_function(args):\n",
        "    random.seed(args.seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    seed_function(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM2GcT6-W_Ax"
      },
      "source": [
        "## official"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0xX70nqdbBT"
      },
      "outputs": [],
      "source": [
        "# ref.: https://github.com/czimaginginstitute/2024_czii_mlchallenge_notebooks/blob/main/3d_unet_monai/train.ipynb\n",
        "# ref.: https://github.com/czimaginginstitute/2024_czii_mlchallenge_notebooks/blob/main/DeepFindET/train.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eoloehs-gd1w"
      },
      "outputs": [],
      "source": [
        "# Creating a copick project\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "'''\n",
        "config_blob = \"\"\"{\n",
        "    \"name\": \"czii_cryoet_mlchallenge_2024\",\n",
        "    \"description\": \"2024 CZII CryoET ML Challenge training data.\",\n",
        "    \"version\": \"1.0.0\",\n",
        "\n",
        "    \"pickable_objects\": [\n",
        "        {\n",
        "            \"name\": \"apo-ferritin\",\n",
        "            \"is_particle\": true,\n",
        "            \"pdb_id\": \"4V1W\",\n",
        "            \"label\": 1,\n",
        "            \"color\": [  0, 117, 220, 128],\n",
        "            \"radius\": 60,\n",
        "            \"map_threshold\": 0.0418\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"beta-amylase\",\n",
        "            \"is_particle\": true,\n",
        "            \"pdb_id\": \"1FA2\",\n",
        "            \"label\": 2,\n",
        "            \"color\": [153,  63,   0, 128],\n",
        "            \"radius\": 65,\n",
        "            \"map_threshold\": 0.035\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"beta-galactosidase\",\n",
        "            \"is_particle\": true,\n",
        "            \"pdb_id\": \"6X1Q\",\n",
        "            \"label\": 3,\n",
        "            \"color\": [ 76,   0,  92, 128],\n",
        "            \"radius\": 90,\n",
        "            \"map_threshold\": 0.0578\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"ribosome\",\n",
        "            \"is_particle\": true,\n",
        "            \"pdb_id\": \"6EK0\",\n",
        "            \"label\": 4,\n",
        "            \"color\": [  0,  92,  49, 128],\n",
        "            \"radius\": 150,\n",
        "            \"map_threshold\": 0.0374\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"thyroglobulin\",\n",
        "            \"is_particle\": true,\n",
        "            \"pdb_id\": \"6SCJ\",\n",
        "            \"label\": 5,\n",
        "            \"color\": [ 43, 206,  72, 128],\n",
        "            \"radius\": 130,\n",
        "            \"map_threshold\": 0.0278\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"virus-like-particle\",\n",
        "            \"is_particle\": true,\n",
        "            \"label\": 6,\n",
        "            \"color\": [255, 204, 153, 128],\n",
        "            \"radius\": 135,\n",
        "            \"map_threshold\": 0.201\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"membrane\",\n",
        "            \"is_particle\": false,\n",
        "            \"label\": 8,\n",
        "            \"color\": [100, 100, 100, 128]\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"background\",\n",
        "            \"is_particle\": false,\n",
        "            \"label\": 9,\n",
        "            \"color\": [10, 150, 200, 128]\n",
        "        }\n",
        "    ],\n",
        "\n",
        "    \"overlay_root\": \"/kaggle/working/overlay\",\n",
        "\n",
        "    \"overlay_fs_args\": {\n",
        "        \"auto_mkdir\": true\n",
        "    },\n",
        "\n",
        "    \"static_root\": \"/kaggle/input/czii-cryo-et-object-identification/train/static\"\n",
        "}\"\"\"\n",
        "'''\n",
        "################################################################################\n",
        "config_blob = \"\"\"{\n",
        "    \"name\": \"czii_cryoet_mlchallenge_2024\",\n",
        "    \"description\": \"2024 CZII CryoET ML Challenge training data.\",\n",
        "    \"version\": \"1.0.0\",\n",
        "\n",
        "    \"pickable_objects\": [\n",
        "        {\n",
        "            \"name\": \"apo-ferritin\",\n",
        "            \"is_particle\": true,\n",
        "            \"pdb_id\": \"4V1W\",\n",
        "            \"label\": 1,\n",
        "            \"color\": [  0, 117, 220, 128],\n",
        "            \"radius\": 60,\n",
        "            \"map_threshold\": 0.0418\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"beta-amylase\",\n",
        "            \"is_particle\": true,\n",
        "            \"pdb_id\": \"1FA2\",\n",
        "            \"label\": 2,\n",
        "            \"color\": [153,  63,   0, 128],\n",
        "            \"radius\": 65,\n",
        "            \"map_threshold\": 0.035\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"beta-galactosidase\",\n",
        "            \"is_particle\": true,\n",
        "            \"pdb_id\": \"6X1Q\",\n",
        "            \"label\": 3,\n",
        "            \"color\": [ 76,   0,  92, 128],\n",
        "            \"radius\": 90,\n",
        "            \"map_threshold\": 0.0578\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"ribosome\",\n",
        "            \"is_particle\": true,\n",
        "            \"pdb_id\": \"6EK0\",\n",
        "            \"label\": 4,\n",
        "            \"color\": [  0,  92,  49, 128],\n",
        "            \"radius\": 150,\n",
        "            \"map_threshold\": 0.0374\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"thyroglobulin\",\n",
        "            \"is_particle\": true,\n",
        "            \"pdb_id\": \"6SCJ\",\n",
        "            \"label\": 5,\n",
        "            \"color\": [ 43, 206,  72, 128],\n",
        "            \"radius\": 130,\n",
        "            \"map_threshold\": 0.0278\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"virus-like-particle\",\n",
        "            \"is_particle\": true,\n",
        "            \"label\": 6,\n",
        "            \"color\": [255, 204, 153, 128],\n",
        "            \"radius\": 135,\n",
        "            \"map_threshold\": 0.201\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"membrane\",\n",
        "            \"is_particle\": false,\n",
        "            \"label\": 8,\n",
        "            \"color\": [100, 100, 100, 128]\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"background\",\n",
        "            \"is_particle\": false,\n",
        "            \"label\": 9,\n",
        "            \"color\": [10, 150, 200, 128]\n",
        "        }\n",
        "    ],\n",
        "\n",
        "    \"overlay_root\": \"/content/overlay\",\n",
        "\n",
        "    \"overlay_fs_args\": {\n",
        "        \"auto_mkdir\": true\n",
        "    },\n",
        "\n",
        "    \"static_root\": \"/content/train/static\"\n",
        "}\"\"\"\n",
        "################################################################################\n",
        "\n",
        "#copick_config_path = \"/kaggle/working/copick.config\"\n",
        "#output_overlay = \"/kaggle/working/overlay\"\n",
        "################################################################################\n",
        "copick_config_path = \"/content/copick.config\"\n",
        "output_overlay = \"/content/overlay\"\n",
        "################################################################################\n",
        "\n",
        "with open(copick_config_path, \"w\") as f:\n",
        "    f.write(config_blob)\n",
        "\n",
        "# Now, setup new overlay directory\n",
        "\n",
        "# Define source and destination directories\n",
        "#source_dir = '/kaggle/input/czii-cryo-et-object-identification/train/overlay'\n",
        "#destination_dir = '/kaggle/working/overlay'\n",
        "################################################################################\n",
        "source_dir = '/content/train/overlay'\n",
        "destination_dir = '/content/overlay'\n",
        "################################################################################\n",
        "\n",
        "# Walk through the source directory\n",
        "for root, dirs, files in os.walk(source_dir):\n",
        "    # Create corresponding subdirectories in the destination\n",
        "    relative_path = os.path.relpath(root, source_dir)\n",
        "    target_dir = os.path.join(destination_dir, relative_path)\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "    # Copy and rename each file\n",
        "    for file in files:\n",
        "        if file.startswith(\"curation_0_\"):\n",
        "            new_filename = file\n",
        "        else:\n",
        "            new_filename = f\"curation_0_{file}\"\n",
        "\n",
        "\n",
        "        # Define full paths for the source and destination files\n",
        "        source_file = os.path.join(root, file)\n",
        "        destination_file = os.path.join(target_dir, new_filename)\n",
        "\n",
        "        # Copy the file with the new name\n",
        "        shutil.copy2(source_file, destination_file)\n",
        "        print(f\"Copied {source_file} to {destination_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTkNrnMhcovp"
      },
      "outputs": [],
      "source": [
        "root = copick.from_file(copick_config_path)\n",
        "\n",
        "copick_user_name = \"copickUtils\"\n",
        "copick_segmentation_name = \"paintedPicks\"\n",
        "voxel_size = 10\n",
        "tomo_type = \"denoised\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YOTLKqDcoxw"
      },
      "outputs": [],
      "source": [
        "from copick_utils.segmentation import segmentation_from_picks\n",
        "import copick_utils.writers.write as write\n",
        "from collections import defaultdict\n",
        "\n",
        "# Just do this once\n",
        "generate_masks = True\n",
        "\n",
        "if generate_masks:\n",
        "    target_objects = defaultdict(dict)\n",
        "    for object in root.pickable_objects:\n",
        "        if object.is_particle:\n",
        "            target_objects[object.name]['label'] = object.label\n",
        "            target_objects[object.name]['radius'] = object.radius\n",
        "\n",
        "\n",
        "    for run in tqdm(root.runs):\n",
        "        tomo = run.get_voxel_spacing(10)\n",
        "        tomo = tomo.get_tomogram(tomo_type).numpy()\n",
        "        target = np.zeros(tomo.shape, dtype=np.uint8)\n",
        "        for pickable_object in root.pickable_objects:\n",
        "            pick = run.get_picks(object_name=pickable_object.name, user_id=\"curation\")\n",
        "            if len(pick):\n",
        "                target = segmentation_from_picks.from_picks(pick[0],\n",
        "                                                            target,\n",
        "                                                            #target_objects[pickable_object.name]['radius'] * 0.8,\n",
        "                                                            ####################\n",
        "                                                            target_objects[pickable_object.name]['radius'] * args.radius_scale,\n",
        "                                                            ####################\n",
        "                                                            target_objects[pickable_object.name]['label']\n",
        "                                                            )\n",
        "        write.segmentation(run, target, copick_user_name, name=copick_segmentation_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p51B7RXEco1K"
      },
      "outputs": [],
      "source": [
        "data_dicts = []\n",
        "for run in tqdm(root.runs):\n",
        "    tomogram = run.get_voxel_spacing(voxel_size).get_tomogram(tomo_type).numpy()\n",
        "    segmentation = run.get_segmentations(name=copick_segmentation_name, user_id=copick_user_name, voxel_size=voxel_size, is_multilabel=True)[0].numpy()\n",
        "    data_dicts.append({\"image\": tomogram, \"label\": segmentation})\n",
        "\n",
        "    ############################################################################\n",
        "    break\n",
        "    ############################################################################\n",
        "\n",
        "print(np.unique(data_dicts[0]['label']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJemQ9Jwc2QA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the images\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Tomogram')\n",
        "plt.imshow(data_dicts[0]['image'][100],cmap='gray')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Painted Segmentation from Picks')\n",
        "plt.imshow(data_dicts[0]['label'][100], cmap='viridis')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U39qprMxd4qj"
      },
      "source": [
        "## preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoCIzEmFd4qj"
      },
      "outputs": [],
      "source": [
        "def resize_function(args, x, mode):\n",
        "    x = torch.tensor(x, dtype = torch.float)\n",
        "    x = x.unsqueeze(0).unsqueeze(0)\n",
        "    x = F.interpolate(x, size = args.volume_size, mode = mode)\n",
        "    x = x.squeeze(0).squeeze(0)\n",
        "    x = x.numpy()\n",
        "    return x\n",
        "\n",
        "def preprocess_function(args):\n",
        "    experiments = sorted(glob.glob('/content/train/static/ExperimentRuns/*'))\n",
        "    experiments = np.array([_.split('/')[-1] for _ in experiments], dtype = str)\n",
        "\n",
        "    cache = {}\n",
        "    for experiment in experiments:\n",
        "        cache[experiment] = {}\n",
        "        for filter in args.filters:\n",
        "            x = zarr.open(args.tomogram_path + f'{experiment}/{args.tomogram_name}/{filter}.zarr')[0][:]\n",
        "            x = resize_function(args, x, mode = 'trilinear')\n",
        "\n",
        "            lower, upper = np.percentile(x, args.clip_range)\n",
        "            x = np.clip(x, lower, upper)\n",
        "\n",
        "            cache[experiment][filter] = x\n",
        "\n",
        "        y = zarr.open(args.segmentation_path + f'{experiment}/{args.segmentation_name}.zarr')[0][:]\n",
        "        y = resize_function(args, y, mode = 'nearest')\n",
        "        cache[experiment]['segmentation'] = y\n",
        "\n",
        "    _experiment = []\n",
        "    _offset = []\n",
        "    for experiment in experiments:\n",
        "        _experiment.extend([experiment] * len(args.offset))\n",
        "        _offset.extend(args.offset)\n",
        "\n",
        "    train = pd.DataFrame()\n",
        "    train['experiment'] = _experiment\n",
        "    train['offset'] = _offset\n",
        "\n",
        "    folds = []\n",
        "\n",
        "    kf = KFold(n_splits = args.n_fold, shuffle = True, random_state = args.seed)\n",
        "    for train_indices, test_indices in kf.split(experiments):\n",
        "\n",
        "        train_experiments = experiments[train_indices]\n",
        "        test_experiments = experiments[test_indices]\n",
        "\n",
        "        train_df = train[train.experiment.isin(train_experiments)].reset_index(drop = True)\n",
        "        test_df = train[train.experiment.isin(test_experiments)].reset_index(drop = True)\n",
        "\n",
        "        folds.append([train_df, test_df])\n",
        "\n",
        "    return train, folds, cache\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train, folds, cache = preprocess_function(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "231WleiyIwf7"
      },
      "source": [
        "## dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPYlBlbvIwf7"
      },
      "outputs": [],
      "source": [
        "class CustomTransform(nn.Module):\n",
        "    def __init__(self, args, is_training):\n",
        "        super(CustomTransform, self).__init__()\n",
        "\n",
        "        if is_training:\n",
        "            self.transform = A.Compose([\n",
        "                A.HorizontalFlip(p = 0.5),\n",
        "                A.VerticalFlip(p = 0.5),\n",
        "                A.Transpose(p = 0.5),\n",
        "\n",
        "                A.ShiftScaleRotate(p = 0.8),\n",
        "\n",
        "                A.RandomBrightnessContrast(p = 0.8),\n",
        "\n",
        "                #A.Resize(height = args.input_size, width = args.input_size, p = 1.0),\n",
        "            ])\n",
        "\n",
        "        else:\n",
        "            self.transform = A.Compose([\n",
        "                #A.Resize(height = args.input_size, width = args.input_size, p = 1.0),\n",
        "            ])\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = x.transpose(1, 2, 0)\n",
        "        y = y.transpose(1, 2, 0)\n",
        "\n",
        "        transformed = self.transform(image = x, mask = y)\n",
        "        x, y = transformed['image'], transformed['mask']\n",
        "\n",
        "        x = x.transpose(2, 0, 1)\n",
        "        y = y.transpose(2, 0, 1)\n",
        "\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pbwxx2MkIwf7"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, args, df, cache, is_training):\n",
        "        self.args = args\n",
        "\n",
        "        self.df = df\n",
        "\n",
        "        self.cache = cache\n",
        "\n",
        "        self.is_training = is_training\n",
        "\n",
        "        self.transform = CustomTransform(args, is_training = is_training)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def get_inputs(self, row):\n",
        "        if self.is_training:\n",
        "            filter = random.choice(self.args.filters)\n",
        "        else:\n",
        "            filter = 'denoised'\n",
        "\n",
        "        inputs = self.cache[row.experiment][filter]\n",
        "        inputs = inputs[\n",
        "            row.offset[0]:row.offset[0] + self.args.patch_size[0],\n",
        "            row.offset[1]:row.offset[1] + self.args.patch_size[1],\n",
        "            row.offset[2]:row.offset[2] + self.args.patch_size[2],\n",
        "        ]\n",
        "\n",
        "        inputs = inputs - np.min(inputs)\n",
        "        inputs = inputs / np.max(inputs)\n",
        "        return inputs\n",
        "\n",
        "    def get_targets(self, row):\n",
        "        targets = self.cache[row.experiment]['segmentation']\n",
        "        targets = targets[\n",
        "            row.offset[0]:row.offset[0] + self.args.patch_size[0],\n",
        "            row.offset[1]:row.offset[1] + self.args.patch_size[1],\n",
        "            row.offset[2]:row.offset[2] + self.args.patch_size[2],\n",
        "        ]\n",
        "        return targets\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.loc[index]\n",
        "\n",
        "        inputs = self.get_inputs(row)\n",
        "        targets = self.get_targets(row)\n",
        "\n",
        "        inputs, targets = self.transform(inputs, targets)\n",
        "\n",
        "        inputs = torch.tensor(inputs, dtype = torch.float)\n",
        "        targets = torch.tensor(targets, dtype = torch.long)\n",
        "        offsets = torch.tensor(row.offset, dtype = torch.long)\n",
        "        return inputs, targets, offsets\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_df, test_df = folds[0]\n",
        "    dataset = CustomDataset(args, train_df, cache, is_training = True)\n",
        "\n",
        "    index = random.randint(0, len(dataset) - 1)\n",
        "    inputs, targets, offsets = dataset[index]\n",
        "\n",
        "    print('inputs : ', inputs.shape)\n",
        "    print('targets : ', targets.shape)\n",
        "    print('offsets : ', offsets)\n",
        "\n",
        "    _, axes = plt.subplots(2, args.patch_size[0], figsize = (args.patch_size[0], 2))\n",
        "    for i in range(args.patch_size[0]):\n",
        "        axes[0, i].imshow(inputs[i], cmap = 'gray')\n",
        "        axes[1, i].imshow(targets[i], cmap = 'viridis')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsegA99cLVHx"
      },
      "source": [
        "## model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUGvgLxMz-vL"
      },
      "outputs": [],
      "source": [
        "%run '/content/drive/MyDrive/Kaggle/CZII - CryoET Object Identification/unet.py'\n",
        "%run '/content/drive/MyDrive/Kaggle/CZII - CryoET Object Identification/3d.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SOSeQJ_LVHy"
      },
      "outputs": [],
      "source": [
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.args = args\n",
        "\n",
        "        self.encoder = timm.create_model(\n",
        "            model_name = args.model_name,\n",
        "            pretrained = True,\n",
        "            features_only = True,\n",
        "            in_chans = args.n_channel,\n",
        "            drop_path_rate = args.drop_path_rate,\n",
        "        )\n",
        "\n",
        "        encoder_channels = [args.n_channel] + [self.encoder.feature_info[i]['num_chs'] for i in range(len(self.encoder.feature_info))]\n",
        "        decoder_channels = args.decoder_channels\n",
        "\n",
        "        self.decoder = UnetDecoder(\n",
        "            encoder_channels = encoder_channels,\n",
        "            decoder_channels = decoder_channels,\n",
        "            n_blocks = args.n_block,\n",
        "            use_batchnorm = True,\n",
        "            center = False,\n",
        "            attention_type = None,\n",
        "        )\n",
        "\n",
        "        self.head = SegmentationHead(\n",
        "            in_channels = decoder_channels[-1],\n",
        "            out_channels = args.n_class,\n",
        "            activation = None,\n",
        "            kernel_size = 3,\n",
        "        )\n",
        "\n",
        "        convert_3d(self.encoder)\n",
        "        convert_3d(self.decoder)\n",
        "        convert_3d(self.head)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        _x = self.encoder(x)\n",
        "        x = self.decoder(*[x] + _x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size = args.batch_size, num_workers = args.n_worker)\n",
        "    sample = next(iter(loader))\n",
        "    sample = [x.to(args.device) for x in sample]\n",
        "\n",
        "    model = CustomModel(args)\n",
        "    model = model.to(args.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with torch.amp.autocast(args.device):\n",
        "            outputs = model(sample[0])\n",
        "            print(outputs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex-SqK4zK6Ml"
      },
      "source": [
        "## model-v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ifchvXtK9b_"
      },
      "outputs": [],
      "source": [
        "%run '/content/drive/MyDrive/Kaggle/CZII - CryoET Object Identification/unet.py'\n",
        "%run '/content/drive/MyDrive/Kaggle/CZII - CryoET Object Identification/3d.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYdsXI1SM0rx"
      },
      "outputs": [],
      "source": [
        "class CustomModelV2(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(CustomModelV2, self).__init__()\n",
        "        self.args = args\n",
        "\n",
        "        self.encoder = timm.create_model(\n",
        "            model_name = args.model_name,\n",
        "            pretrained = True,\n",
        "            features_only = True,\n",
        "            in_chans = args.n_channel,\n",
        "            drop_path_rate = args.drop_path_rate,\n",
        "        )\n",
        "\n",
        "        encoder_channels = [args.n_channel] + [self.encoder.feature_info[i]['num_chs'] for i in range(len(self.encoder.feature_info))]\n",
        "        decoder_channels = args.decoder_channels\n",
        "\n",
        "        self.decoder = UnetDecoder(\n",
        "            encoder_channels = encoder_channels,\n",
        "            decoder_channels = decoder_channels,\n",
        "            n_blocks = args.n_block,\n",
        "            use_batchnorm = True,\n",
        "            center = False,\n",
        "            attention_type = None,\n",
        "        )\n",
        "\n",
        "        self.head = SegmentationHead(\n",
        "            in_channels = decoder_channels[-1],\n",
        "            out_channels = args.n_class,\n",
        "            activation = None,\n",
        "            kernel_size = 3,\n",
        "        )\n",
        "\n",
        "        convert_3d(self.decoder)\n",
        "        convert_3d(self.head)\n",
        "\n",
        "    def get_inputs(self, x):\n",
        "        x = F.pad(\n",
        "            input = x,\n",
        "            pad = (0, 0, 0, 0, (self.args.n_channel - 1)//2, (self.args.n_channel - 1)//2),\n",
        "            mode = 'replicate',\n",
        "        )\n",
        "\n",
        "        x = [x[:, i:i + self.args.n_channel] for i in range(self.args.patch_size[0])]\n",
        "        x = torch.stack(x, dim = 1)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.get_inputs(x)\n",
        "        x = x.reshape(-1, *x.shape[2:])\n",
        "\n",
        "        _x = self.encoder(x)\n",
        "\n",
        "        x = x.reshape(-1, self.args.patch_size[0], *x.shape[1:])\n",
        "        x = x.permute(0, 2, 1, 3, 4)\n",
        "\n",
        "        _x = [_.reshape(-1, self.args.patch_size[0], *_.shape[1:]) for _ in _x]\n",
        "        _x = [_.permute(0, 2, 1, 3, 4) for _ in _x]\n",
        "\n",
        "        x = self.decoder(*[x] + _x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size = args.batch_size, num_workers = args.n_worker)\n",
        "    sample = next(iter(loader))\n",
        "    sample = [x.to(args.device) for x in sample]\n",
        "\n",
        "    model = CustomModelV2(args)\n",
        "    model = model.to(args.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with torch.amp.autocast(args.device):\n",
        "            outputs = model(sample[0])\n",
        "            print(outputs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHds-kgV8HiS"
      },
      "source": [
        "## model-v3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwaBEhyw8HiW"
      },
      "outputs": [],
      "source": [
        "%run '/content/drive/MyDrive/Kaggle/CZII - CryoET Object Identification/unet.py'\n",
        "%run '/content/drive/MyDrive/Kaggle/CZII - CryoET Object Identification/3d.py'\n",
        "%run '/content/drive/MyDrive/Kaggle/CZII - CryoET Object Identification/csn.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgWkKCjR8HiZ"
      },
      "outputs": [],
      "source": [
        "class CustomModelV3(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(CustomModelV3, self).__init__()\n",
        "        self.args = args\n",
        "\n",
        "        self.encoder = ResNet3dCSN(\n",
        "            pretrained2d = False,\n",
        "            pretrained = None,\n",
        "            depth = int(args.model_name[1:-2]),\n",
        "            bottleneck_mode = args.model_name[-2:],\n",
        "            norm_eval = False,\n",
        "            zero_init_residual = False,\n",
        "            in_channels = args.n_channel,\n",
        "            out_indices = np.arange(args.n_block),\n",
        "        )\n",
        "        if args.pretrained_path != None:\n",
        "            self.encoder.init_weights(pretrained = args.pretrained_path)\n",
        "\n",
        "        encoder_channels = [args.n_channel] + args.encoder_channels\n",
        "        decoder_channels = args.decoder_channels\n",
        "\n",
        "        self.decoder = UnetDecoder(\n",
        "            encoder_channels = encoder_channels,\n",
        "            decoder_channels = decoder_channels,\n",
        "            n_blocks = args.n_block,\n",
        "            use_batchnorm = True,\n",
        "            center = False,\n",
        "            attention_type = None,\n",
        "        )\n",
        "\n",
        "        self.head = SegmentationHead(\n",
        "            in_channels = decoder_channels[-1],\n",
        "            out_channels = args.n_class,\n",
        "            activation = None,\n",
        "            kernel_size = 3,\n",
        "        )\n",
        "\n",
        "        convert_3d(self.decoder, pretrained = False)\n",
        "        convert_3d(self.head, pretrained = False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        _x = self.encoder(x)\n",
        "        _x = list(_x)\n",
        "        x = self.decoder(*[x] + _x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size = args.batch_size, num_workers = args.n_worker)\n",
        "    sample = next(iter(loader))\n",
        "    sample = [x.to(args.device) for x in sample]\n",
        "\n",
        "    _args = deepcopy(args)\n",
        "\n",
        "    _args.model_name = 'r50ir'\n",
        "    _args.n_block = 4\n",
        "    _args.encoder_channels = [256, 512, 1024, 2048]\n",
        "    _args.decoder_channels = [256, 128, 64, 32]\n",
        "    _args.pretrained_path = 'https://download.openmmlab.com/mmaction/recognition/csn/ircsn_from_scratch_r50_ig65m_20210617-ce545a37.pth'\n",
        "\n",
        "    model = CustomModelV3(_args)\n",
        "    model = model.to(_args.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with torch.amp.autocast(_args.device):\n",
        "            outputs = model(sample[0])\n",
        "            print(outputs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMqTJ_8gxpb6"
      },
      "source": [
        "## loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xK9oa9ofxpb8"
      },
      "outputs": [],
      "source": [
        "class CustomLoss(nn.Module):\n",
        "    def __init__(self, args, is_training):\n",
        "        super(CustomLoss, self).__init__()\n",
        "        self.args = args\n",
        "\n",
        "        self.is_training = is_training\n",
        "\n",
        "        self.dice_loss = smp.losses.DiceLoss(mode = 'multiclass')\n",
        "\n",
        "        self.focal_loss = smp.losses.FocalLoss(mode = 'multiclass')\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        if self.is_training:\n",
        "            loss1 = self.dice_loss(inputs, targets)\n",
        "            loss2 = self.focal_loss(inputs, targets)\n",
        "\n",
        "            loss = (loss1 + loss2) / 2\n",
        "\n",
        "        else:\n",
        "            loss1 = self.dice_loss(inputs, targets)\n",
        "            loss2 = self.focal_loss(inputs, targets)\n",
        "\n",
        "            loss = (loss1 + loss2) / 2\n",
        "\n",
        "        return loss\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size = args.batch_size, num_workers = args.n_worker)\n",
        "    sample = next(iter(loader))\n",
        "    sample = [x.to(args.device) for x in sample]\n",
        "\n",
        "    if args.model_version == 1:\n",
        "        model = CustomModel(args)\n",
        "    elif args.model_version == 2:\n",
        "        model = CustomModelV2(args)\n",
        "    elif args.model_version == 3:\n",
        "        model = CustomModelV3(args)\n",
        "    else:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    model = model.to(args.device)\n",
        "\n",
        "    loss_function = CustomLoss(args, is_training = True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(sample[0])\n",
        "        loss = loss_function(outputs, sample[1])\n",
        "        print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmh0SHfY6mTV"
      },
      "source": [
        "## utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTKklkw_6mTW"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Derived from:\n",
        "https://github.com/cellcanvas/album-catalog/blob/main/solutions/copick/compare-picks/solution.py\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from scipy.spatial import KDTree\n",
        "\n",
        "\n",
        "class ParticipantVisibleError(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "def compute_metrics(reference_points, reference_radius, candidate_points):\n",
        "    num_reference_particles = len(reference_points)\n",
        "    num_candidate_particles = len(candidate_points)\n",
        "\n",
        "    if len(reference_points) == 0:\n",
        "        return 0, num_candidate_particles, 0\n",
        "\n",
        "    if len(candidate_points) == 0:\n",
        "        return 0, 0, num_reference_particles\n",
        "\n",
        "    ref_tree = KDTree(reference_points)\n",
        "    candidate_tree = KDTree(candidate_points)\n",
        "    raw_matches = candidate_tree.query_ball_tree(ref_tree, r=reference_radius)\n",
        "    matches_within_threshold = []\n",
        "    for match in raw_matches:\n",
        "        matches_within_threshold.extend(match)\n",
        "    # Prevent submitting multiple matches per particle.\n",
        "    # This won't be be strictly correct in the (extremely rare) case where true particles\n",
        "    # are very close to each other.\n",
        "    matches_within_threshold = set(matches_within_threshold)\n",
        "    tp = int(len(matches_within_threshold))\n",
        "    fp = int(num_candidate_particles - tp)\n",
        "    fn = int(num_reference_particles - tp)\n",
        "    return tp, fp, fn\n",
        "\n",
        "\n",
        "#def score(\n",
        "#        solution: pd.DataFrame,\n",
        "#        submission: pd.DataFrame,\n",
        "#        row_id_column_name: str,\n",
        "#        distance_multiplier: float,\n",
        "#        beta: int) -> float:\n",
        "################################################################################\n",
        "def score_function(\n",
        "        solution: pd.DataFrame,\n",
        "        submission: pd.DataFrame,\n",
        "        row_id_column_name: str = 'row_id',\n",
        "        distance_multiplier: float = 0.5,\n",
        "        beta: int = 4) -> float:\n",
        "################################################################################\n",
        "    '''\n",
        "    F_beta\n",
        "      - a true positive occurs when\n",
        "         - (a) the predicted location is within a threshold of the particle radius, and\n",
        "         - (b) the correct `particle_type` is specified\n",
        "      - raw results (TP, FP, FN) are aggregated across all experiments for each particle type\n",
        "      - f_beta is calculated for each particle type\n",
        "      - individual f_beta scores are weighted by particle type for final score\n",
        "    '''\n",
        "\n",
        "    particle_radius = {\n",
        "        'apo-ferritin': 60,\n",
        "        'beta-amylase': 65,\n",
        "        'beta-galactosidase': 90,\n",
        "        'ribosome': 150,\n",
        "        'thyroglobulin': 130,\n",
        "        'virus-like-particle': 135,\n",
        "    }\n",
        "\n",
        "    weights = {\n",
        "        'apo-ferritin': 1,\n",
        "        'beta-amylase': 0,\n",
        "        'beta-galactosidase': 2,\n",
        "        'ribosome': 1,\n",
        "        'thyroglobulin': 2,\n",
        "        'virus-like-particle': 1,\n",
        "    }\n",
        "\n",
        "    particle_radius = {k: v * distance_multiplier for k, v in particle_radius.items()}\n",
        "\n",
        "    # Filter submission to only contain experiments found in the solution split\n",
        "    split_experiments = set(solution['experiment'].unique())\n",
        "    submission = submission.loc[submission['experiment'].isin(split_experiments)]\n",
        "\n",
        "    # Only allow known particle types\n",
        "    if not set(submission['particle_type'].unique()).issubset(set(weights.keys())):\n",
        "        raise ParticipantVisibleError('Unrecognized `particle_type`.')\n",
        "\n",
        "    assert solution.duplicated(subset=['experiment', 'x', 'y', 'z']).sum() == 0\n",
        "    assert particle_radius.keys() == weights.keys()\n",
        "\n",
        "    results = {}\n",
        "    for particle_type in solution['particle_type'].unique():\n",
        "        results[particle_type] = {\n",
        "            'total_tp': 0,\n",
        "            'total_fp': 0,\n",
        "            'total_fn': 0,\n",
        "        }\n",
        "\n",
        "    for experiment in split_experiments:\n",
        "        for particle_type in solution['particle_type'].unique():\n",
        "            reference_radius = particle_radius[particle_type]\n",
        "            select = (solution['experiment'] == experiment) & (solution['particle_type'] == particle_type)\n",
        "            reference_points = solution.loc[select, ['x', 'y', 'z']].values\n",
        "\n",
        "            select = (submission['experiment'] == experiment) & (submission['particle_type'] == particle_type)\n",
        "            candidate_points = submission.loc[select, ['x', 'y', 'z']].values\n",
        "\n",
        "            if len(reference_points) == 0:\n",
        "                reference_points = np.array([])\n",
        "                reference_radius = 1\n",
        "\n",
        "            if len(candidate_points) == 0:\n",
        "                candidate_points = np.array([])\n",
        "\n",
        "            tp, fp, fn = compute_metrics(reference_points, reference_radius, candidate_points)\n",
        "\n",
        "            results[particle_type]['total_tp'] += tp\n",
        "            results[particle_type]['total_fp'] += fp\n",
        "            results[particle_type]['total_fn'] += fn\n",
        "\n",
        "    aggregate_fbeta = 0.0\n",
        "    ############################################################################\n",
        "    fbetas = {}\n",
        "    ############################################################################\n",
        "    for particle_type, totals in results.items():\n",
        "        tp = totals['total_tp']\n",
        "        fp = totals['total_fp']\n",
        "        fn = totals['total_fn']\n",
        "\n",
        "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
        "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
        "        fbeta = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall) if (precision + recall) > 0 else 0.0\n",
        "        ########################################################################\n",
        "        fbetas[particle_type] = f'fbeta:{fbeta:.3f}-recall:{recall:.3f}-precision:{precision:.3f}-tp:{tp}-fp:{fp}-fn:{fn}'\n",
        "        ########################################################################\n",
        "        aggregate_fbeta += fbeta * weights.get(particle_type, 1.0)\n",
        "\n",
        "    if weights:\n",
        "        aggregate_fbeta = aggregate_fbeta / sum(weights.values())\n",
        "    else:\n",
        "        aggregate_fbeta = aggregate_fbeta / len(results)\n",
        "    #return aggregate_fbeta\n",
        "    ############################################################################\n",
        "    fbetas['f-beta'] = aggregate_fbeta\n",
        "    fbetas['n-true'] = len(solution)\n",
        "    fbetas['n-pred'] = len(submission)\n",
        "    return fbetas\n",
        "    ############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQUZyGrt6mTW"
      },
      "outputs": [],
      "source": [
        "def get_solution(args, df):\n",
        "    experiments = list(set(df['experiment']))\n",
        "\n",
        "    _experiment = []\n",
        "    _particle_type = []\n",
        "    _x = []\n",
        "    _y = []\n",
        "    _z = []\n",
        "    for experiment in experiments:\n",
        "        for particle_type in args.particle_types:\n",
        "            with open(f'/content/train/overlay/ExperimentRuns/{experiment}/Picks/{particle_type}.json', \"r\") as f:\n",
        "                pick = json.load(f)\n",
        "\n",
        "                x = [_['location']['x'] for _ in pick['points']]\n",
        "                y = [_['location']['y'] for _ in pick['points']]\n",
        "                z = [_['location']['z'] for _ in pick['points']]\n",
        "\n",
        "                _experiment.extend([experiment] * len(pick['points']))\n",
        "                _particle_type.extend([particle_type] * len(pick['points']))\n",
        "                _x.extend(x)\n",
        "                _y.extend(y)\n",
        "                _z.extend(z)\n",
        "\n",
        "    solution = pd.DataFrame()\n",
        "    solution['experiment'] = _experiment\n",
        "    solution['particle_type'] = _particle_type\n",
        "    solution['x'] = _x\n",
        "    solution['y'] = _y\n",
        "    solution['z'] = _z\n",
        "    return solution\n",
        "\n",
        "def get_submission(args, df, preds, threshold):\n",
        "    experiments = list(df.groupby('experiment', sort = False))\n",
        "    #experiments = [[x[0], len(x[1])] for x in experiments]\n",
        "    ############################################################################\n",
        "    experiments = [[x[0], 184] for x in experiments]\n",
        "    ############################################################################\n",
        "\n",
        "    _experiment = []\n",
        "    _particle_type = []\n",
        "    _x = []\n",
        "    _y = []\n",
        "    _z = []\n",
        "\n",
        "    for experiment, n_slice in experiments:\n",
        "        pred = preds[:n_slice]\n",
        "        for particle_type in args.particle_types:\n",
        "            _class = args.particle2class[particle_type]\n",
        "            _pred = pred[:, _class]\n",
        "\n",
        "            cc = cc3d.connected_components(_pred > threshold)\n",
        "            stats = cc3d.statistics(cc)\n",
        "            zyx = stats['centroids'][1:] * args.voxel_space\n",
        "\n",
        "            #x = zyx[:, 2].tolist()\n",
        "            #y = zyx[:, 1].tolist()\n",
        "            #z = zyx[:, 0].tolist()\n",
        "            ####################################################################\n",
        "            x = (zyx[:, 2] * args.size_scale[2]).tolist()\n",
        "            y = (zyx[:, 1] * args.size_scale[1]).tolist()\n",
        "            z = (zyx[:, 0] * args.size_scale[0]).tolist()\n",
        "            ####################################################################\n",
        "\n",
        "            _experiment.extend([experiment] * zyx.shape[0])\n",
        "            _particle_type.extend([particle_type] * zyx.shape[0])\n",
        "            _x.extend(x)\n",
        "            _y.extend(y)\n",
        "            _z.extend(z)\n",
        "\n",
        "        preds = preds[n_slice:]\n",
        "\n",
        "    submission = pd.DataFrame()\n",
        "    submission['experiment'] = _experiment\n",
        "    submission['particle_type'] = _particle_type\n",
        "    submission['x'] = _x\n",
        "    submission['y'] = _y\n",
        "    submission['z'] = _z\n",
        "    return submission\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    experiments = [_[0] for _ in list(train.groupby('experiment', sort = False))]\n",
        "    for experiment in experiments:\n",
        "        preds = cache[experiment]['segmentation']\n",
        "        preds = torch.tensor(preds, dtype = torch.long)\n",
        "        preds = F.one_hot(preds, num_classes = args.n_class)\n",
        "        preds = preds.permute(0, 3, 1, 2).numpy()\n",
        "\n",
        "        df = train[train.experiment == experiment].reset_index(drop = True)\n",
        "\n",
        "        #args.size_scale = [1.0, 630 / 630, 630 / 630]\n",
        "\n",
        "        solution = get_solution(args, df)\n",
        "        submission = get_submission(args, df, preds, args.thresholds[0])\n",
        "\n",
        "        print(f'{experiment} : {score_function(solution, submission)}')\n",
        "\n",
        "        n_label = [(cache[experiment][\"segmentation\"] == i).sum() for i in range(1, args.n_class)]\n",
        "        print(f'{experiment} : {dict(zip(args.particle_types, n_label))}\\n')\n",
        "\n",
        "        break\n",
        "\n",
        "'''\n",
        "radius_scale = 0.6\n",
        "TS_5_4 : {'apo-ferritin': 'fbeta:1.000-recall:1.000-precision:1.000-tp:46-fp:0-fn:0', 'beta-amylase': 'fbeta:0.900-recall:0.900-precision:0.900-tp:9-fp:1-fn:1', 'beta-galactosidase': 'fbeta:1.000-recall:1.000-precision:1.000-tp:12-fp:0-fn:0', 'ribosome': 'fbeta:1.000-recall:1.000-precision:1.000-tp:31-fp:0-fn:0', 'thyroglobulin': 'fbeta:1.000-recall:1.000-precision:1.000-tp:30-fp:0-fn:0', 'virus-like-particle': 'fbeta:1.000-recall:1.000-precision:1.000-tp:11-fp:0-fn:0', 'f-beta': 1.0, 'n-true': 140, 'n-pred': 140}\n",
        "TS_5_4 : {'apo-ferritin': 9744, 'beta-amylase': 2175, 'beta-galactosidase': 8477, 'ribosome': 102495, 'thyroglobulin': 64065, 'virus-like-particle': 26991}\n",
        "\n",
        "radius_scale = 0.5\n",
        "TS_5_4 : {'apo-ferritin': 'fbeta:1.000-recall:1.000-precision:1.000-tp:46-fp:0-fn:0', 'beta-amylase': 'fbeta:1.000-recall:1.000-precision:1.000-tp:10-fp:0-fn:0', 'beta-galactosidase': 'fbeta:1.000-recall:1.000-precision:1.000-tp:12-fp:0-fn:0', 'ribosome': 'fbeta:1.000-recall:1.000-precision:1.000-tp:31-fp:0-fn:0', 'thyroglobulin': 'fbeta:1.000-recall:1.000-precision:1.000-tp:30-fp:0-fn:0', 'virus-like-particle': 'fbeta:1.000-recall:1.000-precision:1.000-tp:11-fp:0-fn:0', 'f-beta': 1.0, 'n-true': 140, 'n-pred': 140}\n",
        "TS_5_4 : {'apo-ferritin': 5650, 'beta-amylase': 1330, 'beta-galactosidase': 4921, 'ribosome': 59466, 'thyroglobulin': 37126, 'virus-like-particle': 15754}\n",
        "\n",
        "radius_scale = 0.4\n",
        "TS_5_4 : {'apo-ferritin': 'fbeta:1.000-recall:1.000-precision:1.000-tp:46-fp:0-fn:0', 'beta-amylase': 'fbeta:1.000-recall:1.000-precision:1.000-tp:10-fp:0-fn:0', 'beta-galactosidase': 'fbeta:1.000-recall:1.000-precision:1.000-tp:12-fp:0-fn:0', 'ribosome': 'fbeta:1.000-recall:1.000-precision:1.000-tp:31-fp:0-fn:0', 'thyroglobulin': 'fbeta:1.000-recall:1.000-precision:1.000-tp:30-fp:0-fn:0', 'virus-like-particle': 'fbeta:1.000-recall:1.000-precision:1.000-tp:11-fp:0-fn:0', 'f-beta': 1.0, 'n-true': 140, 'n-pred': 140}\n",
        "TS_5_4 : {'apo-ferritin': 2865, 'beta-amylase': 701, 'beta-galactosidase': 2505, 'ribosome': 30490, 'thyroglobulin': 18941, 'virus-like-particle': 8071}\n",
        "\n",
        "radius_scale = 0.3\n",
        "TS_5_4 : {'apo-ferritin': 'fbeta:1.000-recall:1.000-precision:1.000-tp:46-fp:0-fn:0', 'beta-amylase': 'fbeta:1.000-recall:1.000-precision:1.000-tp:10-fp:0-fn:0', 'beta-galactosidase': 'fbeta:1.000-recall:1.000-precision:1.000-tp:12-fp:0-fn:0', 'ribosome': 'fbeta:1.000-recall:1.000-precision:1.000-tp:31-fp:0-fn:0', 'thyroglobulin': 'fbeta:1.000-recall:1.000-precision:1.000-tp:30-fp:0-fn:0', 'virus-like-particle': 'fbeta:1.000-recall:1.000-precision:1.000-tp:11-fp:0-fn:0', 'f-beta': 1.0, 'n-true': 140, 'n-pred': 140}\n",
        "TS_5_4 : {'apo-ferritin': 1198, 'beta-amylase': 297, 'beta-galactosidase': 1068, 'ribosome': 12821, 'thyroglobulin': 7996, 'virus-like-particle': 3425}\n",
        "\n",
        "radius_scale = 0.2\n",
        "TS_5_4 : {'apo-ferritin': 'fbeta:1.000-recall:1.000-precision:1.000-tp:46-fp:0-fn:0', 'beta-amylase': 'fbeta:1.000-recall:1.000-precision:1.000-tp:10-fp:0-fn:0', 'beta-galactosidase': 'fbeta:1.000-recall:1.000-precision:1.000-tp:12-fp:0-fn:0', 'ribosome': 'fbeta:1.000-recall:1.000-precision:1.000-tp:31-fp:0-fn:0', 'thyroglobulin': 'fbeta:1.000-recall:1.000-precision:1.000-tp:30-fp:0-fn:0', 'virus-like-particle': 'fbeta:1.000-recall:1.000-precision:1.000-tp:11-fp:0-fn:0', 'f-beta': 1.0, 'n-true': 140, 'n-pred': 140}\n",
        "TS_5_4 : {'apo-ferritin': 359, 'beta-amylase': 99, 'beta-galactosidase': 311, 'ribosome': 3738, 'thyroglobulin': 2378, 'virus-like-particle': 1018}\n",
        "\n",
        "'''\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3DVjc6X6mTW"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "radius_scale = 0.8\n",
        "TS_5_4 : {'apo-ferritin': 0.8303341902313625, 'beta-amylase': 0.8000000000000002, 'beta-galactosidase': 1.0, 'ribosome': 0.9372623574144486, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 0.9667995068065444}\n",
        "TS_69_2 : {'apo-ferritin': 0.8887015177065767, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 0.9428076256499134, 'virus-like-particle': 1.0, 'f-beta': 0.9677595384294861}\n",
        "TS_6_4 : {'apo-ferritin': 0.9329268292682926, 'beta-amylase': 0.8888888888888888, 'beta-galactosidase': 1.0, 'ribosome': 0.8160000000000001, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 0.9641324041811846}\n",
        "TS_6_6 : {'apo-ferritin': 0.8573487031700289, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 0.9153846153846152, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 0.9675333312220921}\n",
        "TS_73_6 : {'apo-ferritin': 0.90863890615289, 'beta-amylase': 1.0, 'beta-galactosidase': 0.9285714285714285, 'ribosome': 0.9153846153846152, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 0.9544523398114804}\n",
        "TS_86_3 : {'apo-ferritin': 0.8177613320999075, 'beta-amylase': 1.0, 'beta-galactosidase': 0.958974358974359, 'ribosome': 0.8426724137931033, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 0.9397689234059613}\n",
        "TS_99_9 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 0.8814814814814815, 'ribosome': 0.91016333938294, 'thyroglobulin': 0.9603365384615384, 'virus-like-particle': 1.0, 'f-beta': 0.9419713398955685}\n",
        "\n",
        "radius_scale = 0.7\n",
        "TS_5_4 : {'apo-ferritin': 0.9577464788732394, 'beta-amylase': 0.9, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 0.9939637826961771}\n",
        "TS_69_2 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0}\n",
        "TS_6_4 : {'apo-ferritin': 0.9664974619289342, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 0.9474522292993631, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 0.9877070987468997}\n",
        "TS_6_6 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0}\n",
        "TS_73_6 : {'apo-ferritin': 0.9801611903285802, 'beta-amylase': 1.0, 'beta-galactosidase': 0.9285714285714285, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 0.9767577210673482}\n",
        "TS_86_3 : {'apo-ferritin': 0.9696412143514258, 'beta-amylase': 1.0, 'beta-galactosidase': 0.958974358974359, 'ribosome': 0.9646680942184155, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 0.9788940037883656}\n",
        "TS_99_9 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 0.8814814814814815, 'ribosome': 0.970108695652174, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 0.9618673798021623}\n",
        "\n",
        "radius_scale = 0.6\n",
        "TS_5_4 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0}\n",
        "TS_69_2 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0}\n",
        "TS_6_4 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0}\n",
        "TS_6_6 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0}\n",
        "TS_73_6 : {'apo-ferritin': 1.000619578686493, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0000885112409275}\n",
        "TS_86_3 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 0.958974358974359, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 0.9882783882783883}\n",
        "TS_99_9 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 0.875, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 0.9642857142857143}\n",
        "\n",
        "radius_scale = 0.5\n",
        "TS_5_4 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0}\n",
        "TS_69_2 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0}\n",
        "TS_6_4 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0}\n",
        "TS_6_6 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0}\n",
        "TS_73_6 : {'apo-ferritin': 1.000619578686493, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0000885112409275}\n",
        "TS_86_3 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 0.958974358974359, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 0.9882783882783883}\n",
        "TS_99_9 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 0.9583333333333335, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 0.9880952380952381}\n",
        "\n",
        "radius_scale = 0.4\n",
        "TS_5_4 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0}\n",
        "TS_69_2 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0}\n",
        "TS_6_4 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0}\n",
        "TS_6_6 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0}\n",
        "TS_73_6 : {'apo-ferritin': 1.000619578686493, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0000885112409275}\n",
        "TS_86_3 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 0.958974358974359, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 0.9882783882783883}\n",
        "TS_99_9 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0}\n",
        "\n",
        "radius_scale = 0.3\n",
        "TS_5_4 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0}\n",
        "TS_69_2 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0}\n",
        "TS_6_4 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0}\n",
        "TS_6_6 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0}\n",
        "TS_73_6 : {'apo-ferritin': 1.000619578686493, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0000885112409275}\n",
        "TS_86_3 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 0.958974358974359, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 0.9882783882783883}\n",
        "TS_99_9 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0}\n",
        "\n",
        "radius_scale = 0.2\n",
        "TS_5_4 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0}\n",
        "TS_69_2 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0}\n",
        "TS_6_4 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0}\n",
        "TS_6_6 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0}\n",
        "TS_73_6 : {'apo-ferritin': 1.000619578686493, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0000885112409275}\n",
        "TS_86_3 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0}\n",
        "TS_99_9 : {'apo-ferritin': 1.0, 'beta-amylase': 1.0, 'beta-galactosidase': 1.0, 'ribosome': 1.0, 'thyroglobulin': 1.0, 'virus-like-particle': 1.0, 'f-beta': 1.0}\n",
        "'''\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CNrcp6O2W_s"
      },
      "source": [
        "## train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOOSPzGF2ldc"
      },
      "outputs": [],
      "source": [
        "def mixup_function(args, x, y):\n",
        "    _index = torch.randperm(x.shape[0]).to(args.device)\n",
        "    _lambda = np.random.beta(args.mix_alpha, args.mix_alpha)\n",
        "\n",
        "    x = _lambda * x + (1 - _lambda) * x[_index, :]\n",
        "\n",
        "    y1 = y\n",
        "    y2 = y[_index]\n",
        "    return x, (y1, y2), _lambda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y56OyeKi2W_t"
      },
      "outputs": [],
      "source": [
        "class CustomTrainer:\n",
        "    def __init__(self, args, model):\n",
        "        self.model = model\n",
        "\n",
        "        self.scaler = torch.amp.GradScaler(args.device)\n",
        "\n",
        "        if not os.path.exists(args.save_dir):\n",
        "            os.makedirs(args.save_dir)\n",
        "\n",
        "        self.log_path = f'{args.save_dir}/log.txt'\n",
        "\n",
        "        self.optimizer = torch.optim.AdamW(model.parameters(), lr = args.lr, weight_decay = args.wd)\n",
        "\n",
        "        total_steps = args.total_steps\n",
        "        warmup_steps = int(total_steps * args.warmup_ratio)\n",
        "        print('total_steps: ', total_steps)\n",
        "        print('warmup_steps: ', warmup_steps)\n",
        "\n",
        "        self.scheduler = get_cosine_schedule_with_warmup(\n",
        "            self.optimizer,\n",
        "            num_warmup_steps = warmup_steps,\n",
        "            num_training_steps = total_steps,\n",
        "            )\n",
        "\n",
        "        self.train_loss_fn = CustomLoss(args, is_training = True)\n",
        "        self.test_loss_fn = CustomLoss(args, is_training = False)\n",
        "\n",
        "    def run(self, args, train_loader, test_loader):\n",
        "        for epoch in range(args.n_epoch):\n",
        "            lr = self.optimizer.param_groups[0]['lr']\n",
        "            train_loss = self.train_function(args, train_loader)\n",
        "\n",
        "            train_log = f'epoch:{epoch + 1}, lr:{lr}, train_loss:{train_loss:.6f}'\n",
        "            self.log(args, train_log)\n",
        "\n",
        "            if ((epoch + 1) % args.test_freq) == 0:\n",
        "                test_loss, score_dicts = self.test_function(args, test_loader)\n",
        "\n",
        "                test_score = np.max([_[1]['f-beta'] for _ in score_dicts.items()])\n",
        "\n",
        "                test_log = f'epoch:{epoch + 1}, lr:{lr}, test_loss:{test_loss:.6f}, test_score:{test_score:.6f}\\n'\n",
        "                self.log(args, test_log)\n",
        "\n",
        "                score_log = json.dumps(score_dicts, indent = 4)\n",
        "                self.log(args, score_log)\n",
        "\n",
        "                save_path = args.save_dir + '/epoch:' + f'{epoch + 1}'.zfill(3) + \\\n",
        "                            f'-train_loss:{round(train_loss, 6)}' + \\\n",
        "                            f'-test_loss:{round(test_loss, 6)}' + \\\n",
        "                            f'-test_score:{round(test_score, 6)}' + '.bin'\n",
        "                torch.save(self.model.state_dict(), save_path)\n",
        "\n",
        "    def train_function(self, args, loader):\n",
        "        self.model.train()\n",
        "\n",
        "        total_loss = 0.0\n",
        "        for bi, sample in enumerate(tqdm(loader)):\n",
        "            sample = [x.to(args.device) for x in sample]\n",
        "\n",
        "            inputs = sample[0]\n",
        "            targets = sample[1]\n",
        "\n",
        "            mix = (np.random.rand() < args.mix_prob)\n",
        "\n",
        "            if mix:\n",
        "                inputs, targets, _lambda = mixup_function(args, inputs, targets)\n",
        "\n",
        "            with torch.amp.autocast(args.device):\n",
        "                outputs = self.model(inputs)\n",
        "\n",
        "                if mix:\n",
        "                    loss = self.train_loss_fn(outputs, targets[0]) * _lambda + self.train_loss_fn(outputs, targets[1]) * (1 - _lambda)\n",
        "                else:\n",
        "                    loss = self.train_loss_fn(outputs, targets)\n",
        "\n",
        "            loss = loss / args.iters_to_accumulate\n",
        "\n",
        "            self.scaler.scale(loss).backward()\n",
        "            if (bi + 1) % args.iters_to_accumulate == 0:\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                self.scheduler.step()\n",
        "\n",
        "            total_loss += loss.detach().cpu().tolist() * args.iters_to_accumulate\n",
        "\n",
        "        return total_loss/len(loader)\n",
        "\n",
        "    def test_function(self, args, loader):\n",
        "        self.model.eval()\n",
        "\n",
        "        total_loss = 0.0\n",
        "\n",
        "        preds = torch.zeros([args.volume_size[0], args.n_class, args.volume_size[1], args.volume_size[2]], dtype = torch.float16)\n",
        "        trues = torch.zeros([args.volume_size[0], args.volume_size[1], args.volume_size[2]], dtype = torch.long)\n",
        "        counts = torch.zeros([args.volume_size[0], args.volume_size[1], args.volume_size[2]], dtype = torch.long)\n",
        "        for bi, sample in enumerate(tqdm(loader)):\n",
        "            sample = [x.to(args.device) for x in sample]\n",
        "\n",
        "            inputs = sample[0]\n",
        "            targets = sample[1]\n",
        "            offsets = sample[2]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                with torch.amp.autocast(args.device):\n",
        "                    outputs = self.model(inputs)\n",
        "                    loss = self.test_loss_fn(outputs, targets)\n",
        "\n",
        "            total_loss += loss.detach().cpu().tolist()\n",
        "\n",
        "            outputs = torch.softmax(outputs, dim = 1).cpu()\n",
        "            targets = targets.cpu()\n",
        "            offsets = offsets.cpu()\n",
        "\n",
        "            for i in range(offsets.shape[0]):\n",
        "                o1, o2, o3 = offsets[i]\n",
        "\n",
        "                preds[o1:o1 + args.patch_size[0], :, o2:o2 + args.patch_size[1], o3:o3 + args.patch_size[2]] += outputs[i].permute(1, 0, 2, 3)\n",
        "                trues[o1:o1 + args.patch_size[0], o2:o2 + args.patch_size[1], o3:o3 + args.patch_size[2]] += targets[i]\n",
        "                counts[o1:o1 + args.patch_size[0], o2:o2 + args.patch_size[1], o3:o3 + args.patch_size[2]] += 1\n",
        "\n",
        "        preds = preds / counts.unsqueeze(1)\n",
        "        trues = trues / counts\n",
        "\n",
        "        assert cache[list(set(loader.dataset.df.experiment))[0]]['segmentation'].tolist() == trues.tolist()\n",
        "\n",
        "        solution = get_solution(args, loader.dataset.df)\n",
        "\n",
        "        score_dicts = {}\n",
        "        for threshold in args.thresholds:\n",
        "            submission = get_submission(args, loader.dataset.df, preds.numpy(), threshold)\n",
        "\n",
        "            score_dicts[f'f-beta-{threshold:.2f}'] = score_function(solution, submission)\n",
        "\n",
        "        #index = random.randint(0, args.volume_size[0] - 1)\n",
        "        #_, axes = plt.subplots(1, 2, figsize = (5 * 2, 5))\n",
        "        #axes[0].imshow(preds[index].argmax(0), cmap = 'viridis')\n",
        "        #axes[1].imshow(trues[index], cmap = 'viridis')\n",
        "        #plt.show()\n",
        "        ########################################################################\n",
        "        n_plot = 16\n",
        "        assert args.volume_size[0] % n_plot == 0\n",
        "\n",
        "        _, axes = plt.subplots(2, n_plot, figsize = (3 * n_plot, 3 * 2))\n",
        "        for i in range(n_plot):\n",
        "            axes[0, i].imshow(preds[i*(args.volume_size[0]//n_plot)].argmax(0), cmap = 'viridis')\n",
        "            axes[1, i].imshow(trues[i*(args.volume_size[0]//n_plot)], cmap = 'viridis')\n",
        "        plt.show()\n",
        "        ########################################################################\n",
        "\n",
        "        return total_loss/len(loader), score_dicts\n",
        "\n",
        "    def log(self, args, message):\n",
        "        print(message)\n",
        "        with open(f'{args.save_dir}/log.txt', 'a+') as logger:\n",
        "            logger.write(f'{message}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3vXRvnHR_8Q"
      },
      "source": [
        "## run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liu22SfWSA2Z"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    args = CustomConfig()\n",
        "\n",
        "    try: cache\n",
        "    except: train, folds, cache = preprocess_function(args)\n",
        "\n",
        "    for i in [\n",
        "        0, #TS_5_4\n",
        "        #2, #TS_86_3\n",
        "        #4, #TS_73_6\n",
        "        #6, #TS_99_9\n",
        "        ]:\n",
        "        seed_function(args)\n",
        "\n",
        "        train_df, test_df = folds[i]\n",
        "        print('test : ', list(set(test_df['experiment'])))\n",
        "\n",
        "        ########################################################################\n",
        "        #train_df = pd.concat([train_df, test_df], axis = 0)\n",
        "        #train_df = train_df.reset_index(drop = True)\n",
        "        ########################################################################\n",
        "\n",
        "        train_dataset = CustomDataset(args, train_df, cache, is_training = True)\n",
        "        test_dataset = CustomDataset(args, test_df, cache, is_training = False)\n",
        "\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size = args.batch_size,\n",
        "            num_workers = args.n_worker,\n",
        "            shuffle = True,\n",
        "            drop_last = True,\n",
        "            )\n",
        "        test_loader = torch.utils.data.DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size = args.test_batch_size,\n",
        "            num_workers = args.n_worker,\n",
        "            shuffle = False,\n",
        "            drop_last = False,\n",
        "            )\n",
        "\n",
        "        if args.model_version == 1:\n",
        "            model = CustomModel(args)\n",
        "        elif args.model_version == 2:\n",
        "            model = CustomModelV2(args)\n",
        "        elif args.model_version == 3:\n",
        "            model = CustomModelV3(args)\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "        model = model.to(args.device)\n",
        "\n",
        "        name = 'weights/experiment[3d]'\n",
        "        args.save_dir = args.root + name + f'/fold{i + 1}'\n",
        "        args.total_steps = int((len(train_df) // (args.batch_size * args.iters_to_accumulate)) * args.n_epoch)\n",
        "\n",
        "        trainer = CustomTrainer(args, model)\n",
        "        trainer.run(args, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9fexR3QrWre"
      },
      "source": [
        "## test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfkGjq9jrWrh"
      },
      "outputs": [],
      "source": [
        "class CustomWrapper(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(CustomWrapper, self).__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = torch.softmax(x, dim = 1)\n",
        "        return x\n",
        "\n",
        "class CustomEnsemble(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(CustomEnsemble, self).__init__()\n",
        "        self.models = nn.ModuleList()\n",
        "        for config in args.model_weights:\n",
        "            args.model_name = config['model_name']\n",
        "            args.n_block = config['n_block']\n",
        "            args.n_channel = config['n_channel']\n",
        "            args.decoder_channels = [256, 128, 64, 32, 32][:config['n_block']]\n",
        "            args.patch_size = config['patch_size']\n",
        "            args.tta = config['tta']\n",
        "\n",
        "            if config['model_version'] == 1:\n",
        "                model = CustomModel(deepcopy(args))\n",
        "            elif config['model_version'] == 2:\n",
        "                model = CustomModelV2(deepcopy(args))\n",
        "            elif config['model_version'] == 3:\n",
        "                args.encoder_channels = config['encoder_channels']\n",
        "                args.pretrained_path = None\n",
        "                model = CustomModelV3(deepcopy(args))\n",
        "            else:\n",
        "                raise NotImplementedError()\n",
        "\n",
        "            model = model.to(args.device)\n",
        "\n",
        "            checkpoint = torch.load(config['path'], weights_only = True)\n",
        "            model.load_state_dict(checkpoint)\n",
        "            model.eval()\n",
        "\n",
        "            model = CustomWrapper(model)\n",
        "            model = CustomTTA(deepcopy(args), model)\n",
        "\n",
        "            self.models.append(model)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = [model(x) for model in self.models]\n",
        "        x = torch.stack(x, dim = 0)\n",
        "        x = torch.mean(x, dim = 0)\n",
        "        return x\n",
        "\n",
        "class CustomTTA(nn.Module):\n",
        "    def __init__(self, args, model):\n",
        "        super(CustomTTA, self).__init__()\n",
        "        self.args = args\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, x):\n",
        "        _x = 0\n",
        "\n",
        "        if 'original' in self.args.tta:\n",
        "            _x += self.model(x)\n",
        "\n",
        "        if 'horizontal' in self.args.tta:\n",
        "            _x += self.model(x.flip(3)).flip(4)\n",
        "\n",
        "        if 'vertical' in self.args.tta:\n",
        "            _x += self.model(x.flip(2)).flip(3)\n",
        "\n",
        "        if 'transpose' in self.args.tta:\n",
        "            _x += self.model(x.permute(0, 1, 3, 2)).permute(0, 1, 2, 4, 3)\n",
        "\n",
        "        if 'rotate90' in self.args.tta:\n",
        "            _x += self.model(x.rot90(k = 1, dims = (2, 3))).rot90(k = -1, dims = (3, 4))\n",
        "\n",
        "        if 'rotate180' in self.args.tta:\n",
        "            _x += self.model(x.rot90(k = 2, dims = (2, 3))).rot90(k = -2, dims = (3, 4))\n",
        "\n",
        "        if 'rotate270' in self.args.tta:\n",
        "            _x += self.model(x.rot90(k = 3, dims = (2, 3))).rot90(k = -3, dims = (3, 4))\n",
        "\n",
        "        _x = _x / len(self.args.tta)\n",
        "        return _x\n",
        "\n",
        "def get_outputs(args, model, loader):\n",
        "    model.eval()\n",
        "\n",
        "    preds = torch.zeros([args.volume_size[0], args.n_class, args.volume_size[1], args.volume_size[2]], dtype = torch.float16, device = args.device)\n",
        "    trues = torch.zeros([args.volume_size[0], args.volume_size[1], args.volume_size[2]], dtype = torch.long, device = args.device)\n",
        "    counts = torch.zeros([args.volume_size[0], args.volume_size[1], args.volume_size[2]], dtype = torch.long, device = args.device)\n",
        "    for bi, sample in enumerate(tqdm(loader)):\n",
        "        sample = [x.to(args.device) for x in sample]\n",
        "\n",
        "        inputs = sample[0]\n",
        "        targets = sample[1]\n",
        "        offsets = sample[2]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with torch.amp.autocast(args.device):\n",
        "                outputs = model(inputs)\n",
        "\n",
        "        for i in range(offsets.shape[0]):\n",
        "            o1, o2, o3 = offsets[i]\n",
        "\n",
        "            preds[o1:o1 + args.patch_size[0], :, o2:o2 + args.patch_size[1], o3:o3 + args.patch_size[2]] += outputs[i].permute(1, 0, 2, 3)\n",
        "            trues[o1:o1 + args.patch_size[0], o2:o2 + args.patch_size[1], o3:o3 + args.patch_size[2]] += targets[i]\n",
        "            counts[o1:o1 + args.patch_size[0], o2:o2 + args.patch_size[1], o3:o3 + args.patch_size[2]] += 1\n",
        "\n",
        "    preds = preds / counts.unsqueeze(1)\n",
        "    trues = trues / counts\n",
        "\n",
        "    preds = preds.cpu()\n",
        "    trues = trues.cpu()\n",
        "    return preds, trues\n",
        "\n",
        "def inference_function(args, models, experiment, threshold):\n",
        "    n_ensemble, preds = 0, 0\n",
        "    for config in args.configs:\n",
        "        args.patch_size = config['patch_size']\n",
        "        args.offset = config['offset']\n",
        "\n",
        "        df = pd.DataFrame()\n",
        "        df['experiment'] = [experiment] * len(args.offset)\n",
        "        df['offset'] = args.offset\n",
        "\n",
        "        dataset = CustomDataset(args, df, cache, is_training = False)\n",
        "        loader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_size = config['batch_size'],\n",
        "            num_workers = args.n_worker,\n",
        "            shuffle = False,\n",
        "            drop_last = False\n",
        "            )\n",
        "\n",
        "        _preds, trues = get_outputs(args, models[config['data_version']], loader)\n",
        "\n",
        "        n_ensemble += len(config['model_weights'])\n",
        "        preds += len(config['model_weights']) * _preds\n",
        "\n",
        "    preds = preds / n_ensemble\n",
        "\n",
        "    solution = get_solution(args, loader.dataset.df)\n",
        "    submission = get_submission(args, loader.dataset.df, preds.numpy(), threshold)\n",
        "    return solution, submission, preds\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = CustomConfig()\n",
        "\n",
        "    tta = np.array([\n",
        "        'original',\n",
        "        'vertical',\n",
        "        'transpose',\n",
        "        'rotate90',\n",
        "        'rotate270',\n",
        "    ])\n",
        "\n",
        "    args.configs = [\n",
        "        {\n",
        "            'data_version' : 1,\n",
        "            'patch_size' : [64, 256, 256],\n",
        "            'offset' : [[i, j, k] for i in [0, 42, 84, 128] for j in [0, 192, 384] for k in [0, 192, 384]],\n",
        "            'batch_size' : 1 * 3,\n",
        "            'model_weights' : [\n",
        "                {\n",
        "                    'model_name' : 'resnet34d.ra2_in1k',\n",
        "                    'n_block' : 5,\n",
        "                    'model_version' : 1,\n",
        "                    'n_channel' : 1,\n",
        "                    'path' : args.root + 'weights/main/model[resnet34d]-volume_size[64,256,256]-clip[1]-radius_scale[0.5]-mixup[0.50]/fold1/epoch:030-train_loss:0.247588-test_loss:0.226862-test_score:0.775869.bin',\n",
        "                    'patch_size' : [64, 256, 256],\n",
        "                    'tta' : tta[[0, 1]],\n",
        "                },\n",
        "                {\n",
        "                    'model_name' : 'tf_efficientnet_b1.in1k',\n",
        "                    'n_block' : 5,\n",
        "                    'model_version' : 2,\n",
        "                    'n_channel' : 3,\n",
        "                    'path' : args.root + 'weights/main/model[tf_efficientnet_b1]-volume_size[64,3,256,256]-clip[1]-radius_scale[0.5]-mixup[0.50]/fold1/epoch:026-train_loss:0.247768-test_loss:0.218416-test_score:0.775452.bin',\n",
        "                    'patch_size' : [64, 256, 256],\n",
        "                    'tta' : tta[[0, 2]],\n",
        "                },\n",
        "                {\n",
        "                    'model_name' : 'r152ir',\n",
        "                    'n_block' : 4,\n",
        "                    'model_version' : 3,\n",
        "                    'n_channel' : 1,\n",
        "                    'encoder_channels' : [256, 512, 1024, 2048],\n",
        "                    'path' : args.root + 'weights/main/model[r152ir]-volume_size[64,256,256]-clip[1]-radius_scale[0.5]-mixup[0.50]/fold1/epoch:030-train_loss:0.239091-test_loss:0.219818-test_score:0.759626.bin',\n",
        "                    'patch_size' : [64, 256, 256],\n",
        "                    'tta' : tta[[0, 3]],\n",
        "                },\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            'data_version' : 2,\n",
        "            'patch_size' : [32, 352, 352],\n",
        "            'offset' : [[i, j, k] for i in [0, 20, 40, 60, 80, 100, 120, 140, 160] for j in [0, 288] for k in [0, 288]],\n",
        "            'batch_size' : 1 * 3,\n",
        "            'model_weights' : [\n",
        "                {\n",
        "                    'model_name' : 'resnet18d.ra2_in1k',\n",
        "                    'n_block' : 5,\n",
        "                    'model_version' : 1,\n",
        "                    'n_channel' : 1,\n",
        "                    'path' : args.root + 'weights/main/model[resnet18d]-volume_size[32,352,352]-clip[1]-radius_scale[0.5]-mixup[0.50]/fold1/epoch:024-train_loss:0.259471-test_loss:0.215104-test_score:0.772145.bin',\n",
        "                    'patch_size' : [32, 352, 352],\n",
        "                    'tta' : tta[[0, 4]],\n",
        "                },\n",
        "                {\n",
        "                    'model_name' : 'resnet18d.ra2_in1k',\n",
        "                    'n_block' : 5,\n",
        "                    'model_version' : 2,\n",
        "                    'n_channel' : 3,\n",
        "                    'path' : args.root + 'weights/main/model[resnet18d]-volume_size[32,3,352,352]-clip[1]-radius_scale[0.5]-mixup[0.50]/fold1/epoch:028-train_loss:0.244331-test_loss:0.200117-test_score:0.726298.bin',\n",
        "                    'patch_size' : [32, 352, 352],\n",
        "                    'tta' : tta[[1, 2]],\n",
        "                },\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            'data_version' : 3,\n",
        "            'patch_size' : [32, 224, 224],\n",
        "            'offset' : [[i, j, k] for i in [0, 23, 46, 69, 92, 115, 138, 160] for j in [0, 208, 416] for k in [0, 208, 416]],\n",
        "            'batch_size' : 1 * 4,\n",
        "            'model_weights' : [\n",
        "                {\n",
        "                    'model_name' : 'resnet18d.ra2_in1k',\n",
        "                    'n_block' : 5,\n",
        "                    'model_version' : 1,\n",
        "                    'n_channel' : 1,\n",
        "                    'path' : args.root + 'weights/main/model[resnet18d]-volume_size[32,224,224]-clip[1]-radius_scale[0.5]-mixup[0.50]/fold1/epoch:028-train_loss:0.26338-test_loss:0.188437-test_score:0.759627.bin',\n",
        "                    'patch_size' : [32, 224, 224],\n",
        "                    'tta' : tta[[1, 3]],\n",
        "                },\n",
        "                {\n",
        "                    'model_name' : 'resnet18d.ra2_in1k',\n",
        "                    'n_block' : 5,\n",
        "                    'model_version' : 2,\n",
        "                    'n_channel' : 3,\n",
        "                    'path' : args.root + 'weights/main/model[resnet18d]-volume_size[32,3,224,224]-clip[1]-radius_scale[0.5]-mixup[0.50]/fold1/epoch:030-train_loss:0.255669-test_loss:0.183632-test_score:0.762387.bin',\n",
        "                    'patch_size' : [32, 224, 224],\n",
        "                    'tta' : tta[[1, 4]],\n",
        "                },\n",
        "                {\n",
        "                    'model_name' : 'r50ir',\n",
        "                    'n_block' : 4,\n",
        "                    'model_version' : 3,\n",
        "                    'n_channel' : 1,\n",
        "                    'encoder_channels' : [256, 512, 1024, 2048],\n",
        "                    'path' : args.root + 'weights/main/model[r50ir]-volume_size[32,224,224]-clip[1]-radius_scale[0.5]-mixup[0.50]/fold1/epoch:030-train_loss:0.249857-test_loss:0.184979-test_score:0.756809.bin',\n",
        "                    'patch_size' : [32, 224, 224],\n",
        "                    'tta' : tta[[2, 3]],\n",
        "                },\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            'data_version' : 4,\n",
        "            'patch_size' : [32, 128, 128],\n",
        "            'offset' : [[i, j, k] for i in [0, 27, 54, 81, 108, 135, 160] for j in [0, 102, 204, 306, 408, 512] for k in [0, 102, 204, 306, 408, 512]],\n",
        "            'batch_size' : 1 * 3,\n",
        "            'model_weights' : [\n",
        "                {\n",
        "                    'model_name' : 'resnet18d.ra2_in1k',\n",
        "                    'n_block' : 5,\n",
        "                    'model_version' : 1,\n",
        "                    'n_channel' : 1,\n",
        "                    'path' : args.root + 'weights/main/model[resnet18d]-volume_size[32,128,128]-clip[1]-radius_scale[0.5]-mixup[0.50]-n_epoch[40]/fold1/epoch:034-train_loss:0.256083-test_loss:0.192153-test_score:0.786046.bin',\n",
        "                    'patch_size' : [32, 128, 128],\n",
        "                    'tta' : tta[[2, 4]],\n",
        "                },\n",
        "                {\n",
        "                    'model_name' : 'resnet18d.ra2_in1k',\n",
        "                    'n_block' : 5,\n",
        "                    'model_version' : 2,\n",
        "                    'n_channel' : 3,\n",
        "                    'path' : args.root + 'weights/main/model[resnet18d]-volume_size[32,3,128,128]-clip[1]-radius_scale[0.5]-mixup[0.50]-n_epoch[40]/fold1/epoch:032-train_loss:0.257508-test_loss:0.187082-test_score:0.793733.bin',\n",
        "                    'patch_size' : [32, 128, 128],\n",
        "                    'tta' : tta[[3, 4]],\n",
        "                },\n",
        "            ]\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    args.thresholds = [\n",
        "        #0.90,\n",
        "        #0.75,\n",
        "        #0.50,\n",
        "        0.25,\n",
        "        #0.10,\n",
        "        ]\n",
        "\n",
        "    seed_function(args)\n",
        "\n",
        "    experiment = 'TS_5_4'\n",
        "\n",
        "    models = {}\n",
        "    for config in args.configs:\n",
        "        args.model_weights = config['model_weights']\n",
        "\n",
        "        model = CustomEnsemble(args)\n",
        "\n",
        "        models[config['data_version']] = model\n",
        "\n",
        "    for threshold in args.thresholds:\n",
        "        solution, submission, preds = inference_function(args, models, experiment, threshold)\n",
        "\n",
        "        score = score_function(solution, submission)\n",
        "        print(f'threshold : {threshold}, score : {score}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "d-cNuu_WMuwV",
        "hQBAR1gnMyS2",
        "VuemSOaegL1d",
        "Yqi9nGb9PosD",
        "JM2GcT6-W_Ax",
        "U39qprMxd4qj",
        "231WleiyIwf7",
        "GsegA99cLVHx",
        "ex-SqK4zK6Ml",
        "pHds-kgV8HiS",
        "PMqTJ_8gxpb6",
        "Dmh0SHfY6mTV",
        "5CNrcp6O2W_s",
        "s3vXRvnHR_8Q",
        "j9fexR3QrWre"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}